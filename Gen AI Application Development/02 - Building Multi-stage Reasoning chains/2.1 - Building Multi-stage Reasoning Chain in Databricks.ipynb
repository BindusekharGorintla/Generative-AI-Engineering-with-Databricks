{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adad02b8-4467-4e8a-aa65-7b4e9a2351c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7ae1968-6d75-4130-9f54-7b356d75a2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Building Multi-stage Reasoning Chain in Databricks\n",
    "\n",
    "In this demo we will start building a multi-stage reasoning system using Databricks' features and LangChain. Before we build the chain, first, we will show various components that are commonly used in multi-stage chaining system. \n",
    "\n",
    "In the main section of the demo, we will build a multi-stage system. First, we will build a chain that will answer user questions using `llama-3` model. The second chain will search for DAIS-2023 talks and will try to find the corresponding video on YouTube. The final, complete chain will recommend videos to the user.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Identify that LangChain can include stages/tasks that are not LLMs.\n",
    "\n",
    "* Create basic LLM chains to connect prompts and LLMs.\n",
    "\n",
    "* Use tools to complete various tasks in the complete system.\n",
    "\n",
    "* Construct sequential chains of multiple LLMChains to perform multi-stage reasoning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe41df6-be2a-4dac-8661-f23b9824ec84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb66f56-587b-4b65-889a-a0d665ca5554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6eb0bc-43e0-405b-ba35-86b1ccbf57b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06973ddf-9b56-4696-8dcf-a1deadd9271b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq databricks-sdk databricks-vectorsearch==0.60 'mlflow-skinny[databricks]==3.4.0' databricks-langchain==0.8.0 langchain==0.3.7 langchain-community==0.3.7 grandalf youtube_search==2.1.2 Wikipedia==1.4.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f48fe0-5b82-4322-8b0b-9da693087804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fcf2244-9bf2-4577-abb7-d8a7b2e5e633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57623f3-1625-45e5-bab6-08591c5eed15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f05fa6-7ef7-4df2-9071-08112efeedfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using LLMs and Prompts without an External Library\n",
    "\n",
    "While there are many libraries out there for building chains and in this demo we will be using one as well, you don't need a third party library for a simple prompt. We can simply use `databricks-sdk` to directly query a **Foundational Model API endpoint**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3d4020-3553-4f65-a702-9603253a11e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "genre = \"Action\"\n",
    "actor = \"Tom Cruise\"\n",
    "\n",
    "prompt = f\"Tell me about a {genre} movie which {actor} is one of the actors.\"\n",
    "\n",
    "messages = [\n",
    "    { \n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt \n",
    "    }\n",
    "]\n",
    "messages = [ChatMessage.from_dict(message) for message in messages]\n",
    "llm_response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    messages=messages,\n",
    "    temperature=0.2,\n",
    "    max_tokens=128\n",
    ")\n",
    "\n",
    "print(llm_response.as_dict()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1c7231-0274-4651-94b6-2c0ba98acf24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## LangChain Basics\n",
    "\n",
    "As demonstrated in the previous section, **it is not necessary to use a third-party chaining library** to construct a multi-chain AI system. However, composition libraries like **LangChain** can simplify some of the steps by providing a generic interface for supported large language models (LLMs).\n",
    "\n",
    "Before we begin building a multi-stage chain, let's review the main LangChain components that we will use in this module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6714a5a-1120-42d3-a825-3b7d937ff5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prompt\n",
    "\n",
    "Prompt is one of the basic blocks when interacting with GenAI models. They may include instructions, examples and specific context information related to the given task. Let's create a very basic prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f136124e-d71c-49c9-8682-a64e8b8cd068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me about a {genre} movie which {actor} is one of the actors.\")\n",
    "prompt_template.format(genre=\"Action\", actor=\"Tom Cruise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be37562-69a8-4ca7-9792-bce8a0f7a89d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LLMs\n",
    "\n",
    "LLMs are the core component when building compound AI systems. They are the **brain** of the system for reasoning and generating the response. The [list of supported LLMs](https://python.langchain.com/v0.2/docs/integrations/chat/) can be found on LangChain documentation. \n",
    "\n",
    "Let's see how to interact with **Llama-3** model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e8062d-3277-478e-9cd6-02ed853588ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# You can play with max_tokens to define the length of the response\n",
    "llm_llama = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 500)\n",
    "\n",
    "for chunk in llm_llama.stream(\"Who is Tom Cruise?\"):\n",
    "    print(chunk.content, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e0bf7f-b9e2-4df6-b78e-773d7b91de55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Retriever\n",
    "\n",
    "Retrievers accept a string as input and return the list of documents. They are mainly used for retrieving external data and passing it to the model for generating response. Typically, retrievers don't use LLMs instead they are using similarity search under the hood to find the most similar documents. There are various types of retrievers such as *document retrievers* and *vector store retrievers*. The [list of supported retrievers](https://python.langchain.com/v0.2/docs/integrations/retrievers/) can be found on LangChain documentation. \n",
    "\n",
    "In the next section of the demo, we will use **Databricks Vector Search** as retriever to fetch documents by input query.\n",
    "\n",
    "For now, let's try a simple **Wikipedia retriever**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759d20ac-d548-4e4c-abe3-bf181381f386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "retriever = WikipediaRetriever()\n",
    "docs = retriever.invoke(input=\"Tom Cruise\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0406d7bc-06e0-404e-be5d-3a06d4ffc46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tools\n",
    "\n",
    "Tools are functions that can be invoked in the chain. Tools has *input parameters* and a *function* to run. The [list of supported tools](https://python.langchain.com/v0.2/docs/integrations/tools/) can be found on LangChain documentation. \n",
    "\n",
    "Here, we have a Youtube search tool. The tool's `description` defines why a tool can be used and the `args` defines what input arguments can be passed to the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db762d7-2a96-48d5-9128-759143731c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "tool = YouTubeSearchTool()\n",
    "tool.run(\"Tom Cruise movie trailer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09eae44-264e-463a-9091-225c958b0e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(tool.description)\n",
    "print(tool.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a4d802-33bf-4e19-b6b7-3d339eb50927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Chaining\n",
    "\n",
    "One of the important features of these components is the ability to **chain** them together. Let's connect the LLM with the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833f0ed6-73da-4b23-b6b0-31f70a759463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt_template | llm_llama | StrOutputParser()\n",
    "print(chain.invoke({\"genre\":\"Action\", \"actor\":\"Tom Cruise\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858005da-a19a-4899-9384-e83901ce1c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Build a Multi-stage Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933e93c3-603a-4a0f-977a-575689ac0220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enable MLflow Auto-Log\n",
    "\n",
    "MLflow has support for auto-logging LangChain models. Auto-logging is enabled by default. If not enabled, we would enable auto-logging as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6f2145-ea2c-4922-b8ef-a8bc88c442b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2f77bde-d6b1-45a6-8556-29bcb93aaeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Vector Store\n",
    "\n",
    "This course does not cover vector stores in depth, as they fall outside its scope. For a comprehensive understanding of vector stores, including the Databricks Vector Store, please refer to the **Generative AI Solution Development** course. \n",
    "\n",
    "**\uD83D\uDEA8IMPORTANT: Vector Search endpoints must be created before running the rest of the demo. These are already created for you in Databricks Lab environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd97d861-3b32-4571-a9f9-b65b49f7db08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign VS search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "\n",
    "vs_endpoint_name = vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")\n",
    "\n",
    "# Source table and VS index table names\n",
    "vs_index_table_fullname = f\"{DA.catalog_name}.{DA.schema_name}.dais_embeddings\"\n",
    "source_table_fullname = f\"{DA.catalog_name}.{DA.schema_name}.dais_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a718e911-5e56-4603-b17f-bd116ccf292a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The dataset that we will be using in this module is already created in the classroom setup script. Let's have a quick look at the dataset.\n",
    "\n",
    "Then, we will create a vector store index and store embeddings there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f473ec-b94a-4974-b882-afa3925cc864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM {source_table_fullname}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca00a0d-554b-4668-9732-13fa1cf492e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store embeddings in vector store\n",
    "create_vs_index(vs_endpoint_name, vs_index_table_fullname, source_table_fullname, \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9727cb4c-87af-442b-8fb1-a29ba7443516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Common Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63003a9c-5460-413a-bb83-feb8b3a3ef8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "llm_llama = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76bc7c65-a923-4c3d-92ed-c1ef30cbf25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build First Chain\n",
    "\n",
    "The **first chain** will be used for listing videos relevant to the user's question. In order to get videos, first, we need to search for the DAIS-2023 talks that are already stored in a Vector Search index. After retrieving the relevant titles, we will use YouTube search tool to get the videos for the talks. In the final stage, these videos are passed to the chain to generate a response for the user.\n",
    "\n",
    "This chain consist of a `prompt template`, `retriever`, `llm model` and `output parser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a9fcda-5c5c-4285-b860-7795c6ee91f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "# Prompt to format the video titles for YouTube search\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Construct a search query for YouTube based on the titles below. Make sure to add DAIS 2023 as part of the query. Remove all quotes from the query and return only the query.\n",
    "\n",
    "    <video_titles>\n",
    "    {context}\n",
    "    </video_titles>\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a vector store client and retrieve documents\n",
    "def get_retriever(persist_dir=None):\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "    vs_index = vsc.get_index(vs_endpoint_name, vs_index_table_fullname)\n",
    "    vectorstore = DatabricksVectorSearch(vs_index_table_fullname)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "# First chain\n",
    "chain_video = (\n",
    "    {\"context\": get_retriever(), \"input\": RunnablePassthrough()}\n",
    "    | prompt_template_1\n",
    "    | llm_llama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "chain_video.invoke(\"How machine learning models are stored in Unity Catalog?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f91ba7c-bf77-41dc-9a10-352813279d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build Second Chain\n",
    "\n",
    "This chain will use the video title-based query from the previous chain to search YouTube. The response from this chain will consist of YouTube video links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e72161b1-48d1-4579-9b5a-b430ceb79045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Generate image using first chain\n",
    "def get_videos(input):\n",
    "    tool_yt = YouTubeSearchTool()\n",
    "    video_urls = tool_yt.run(input)\n",
    "    return video_urls\n",
    "\n",
    "chain_youtube = RunnableLambda(get_videos) | StrOutputParser()\n",
    "\n",
    "# Get the image URL\n",
    "response = chain_youtube.invoke(\"DAIS 2023 Streamlining API Deployment for ML Models Across Multiple Brands Ahold Delhaize's Experience on Serverless OR Building a Real-Time Model Monitoring Pipeline on Databricks\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6942d64d-0f17-4892-849e-8a34474c0982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build Third Chain\n",
    "\n",
    "The **third chain** will be a simple question-answer prompt using **Meta's Llama-3**. The chain will use the video links as well for recommendation. This chain consist of a `prompt template`, `llm model` and `output parser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fdb0f9-bb5a-4d66-8897-4a1281472cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_template_3 = PromptTemplate.from_template(\n",
    "    \"\"\"You are a Databricks expert. You will get questions about Databricks. Try to give simple answers and be professional. Don't include code in your response.\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Answer:\n",
    "\n",
    "    Also, encourage the user to watch the videos provided below. Show video links as a list. Strip the YouTube link at \"&pp=\" and keep the first part of the URL. There is no title for the links so only show the URL. Only use the videos provided below.\n",
    "\n",
    "    Video Links: {videos}\n",
    "\n",
    "    Format response in HTML format.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain_expert = (prompt_template_3 | llm_llama | StrOutputParser())\n",
    "chain_expert.invoke({\n",
    "    \"input\": \"How machine learning models are stored in Unity Catalog?\",\n",
    "    \"videos\": \"\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "346832ec-2773-4ad8-8cb0-93856e252c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Chaining Chains ⛓️\n",
    "\n",
    "So far we create chains for each stage. To build a multi-stage system, we need to link these chains together and build a multi-chain system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d5bacf-98dd-4ac7-9ae4-2884721bf6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "multi_chain = (\n",
    "  {\n",
    "    \"input\": RunnablePassthrough(),\n",
    "    \"videos\": (chain_video | chain_youtube | StrOutputParser())\n",
    "  }\n",
    "  |chain_expert\n",
    "  |StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"How machine learning models are stored in Unity Catalog?\"\n",
    "response = multi_chain.invoke(query)\n",
    "display(HTML(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ccc7b4-49fb-4a66-8517-dc149cfa0d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View the flow of the final chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f424cb0-67fc-491e-89da-ba3f0262a6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "multi_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e780426c-ebce-46be-a7f8-2a7dfd57e5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save the Chain to Model Registry in UC\n",
    "\n",
    "Now that our chain is ready and evaluated, we can register it within our Unity Catalog schema. \n",
    "\n",
    "After registering the chain, you can view the chain and models in the **Catalog Explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d19b3c90-1bea-4e48-99c0-2b9f305602ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# Set model registry to UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.multi_stage_demo\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"multi_stage_demo\") as run:\n",
    "    signature = infer_signature(query, response)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        multi_chain,\n",
    "        loader_fn=get_retriever, \n",
    "        name=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        input_example=query,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69f27ddf-5ed8-4725-93e7-fa3618fc536e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the chain from Model Registry in UC\n",
    "\n",
    "Now that our chain is registered in UC, we can load it and invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafdc13d-4c13-46b9-89a8-be46109174e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"models:/{model_name}/{model_info.registered_model_version}\"\n",
    "model = mlflow.langchain.load_model(model_uri)\n",
    "\n",
    "model.invoke(\"How machine learning models are stored in Unity Catalog?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68e338a-b653-4c92-9269-f336d65c0eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we explored building a multi-stage reasoning system with Databricks' tools and LangChain. We began by introducing common system components and then focused on creating chains for specific tasks like answering user queries and finding DAIS-2023 talks. By the end, participants learned to use LangChain beyond just LLMs and construct sequential chains for multi-stage analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba48ad3-eaf7-4889-ac0b-5e73834b2248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.1 - Building Multi-stage Reasoning Chain in Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}