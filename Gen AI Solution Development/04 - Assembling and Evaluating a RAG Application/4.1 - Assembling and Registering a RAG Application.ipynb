{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c76634f7-e471-4ca3-a146-9a8c572a4155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa191f50-c75b-4015-b12c-8be12237d114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Assembling and Registering a RAG Application\n",
    "\n",
    "In the previous demo, we created a Vector Search Index. To build a complete RAG application, it is time to connect all the components that you have learned so far and evaluate the performance of the RAG.\n",
    "\n",
    "After evaluating the performance of the RAG pipeline, we will create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "- Describe embeddings, vector databases, and search/retrieval as key components of implementing performant RAG applications.\n",
    "- Assemble a RAG pipeline by combining various components.\n",
    "- Build a RAG evaluation pipeline with MLflow evaluation functions.\n",
    "- Register a RAG pipeline to the Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5d93de-4c33-4cde-bb77-404d8a488ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b356467b-33f3-46dc-a8c2-bef02228c2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**\n",
    "\n",
    "\n",
    "\n",
    "**\uD83D\uDEA8 Important: This demonstration relies on the resources established in the previous one. Please ensure you have completed the prior demonstration before starting this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06dd596a-54f9-496d-b752-e66da0c92a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3d689d-8adf-4592-9d11-c76cdff706d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqq databricks-sdk databricks-vectorsearch 'mlflow-skinny[databricks]==3.4.0' langchain==0.3.26 databricks-langchain==0.8.0 PyPDF2==3.0.0 flashrank\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e3e387-5d2f-4f36-849e-ba4c8d9c07e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4a05f5-5767-4434-a625-b42f683cc668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5515cae-a59b-4a4e-9408-dcd456ab47ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468ecaf4-742d-452e-9f90-1a13231aa9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser12696561_1763456832@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser12696561_1763456832\nWorking Directory: /Volumes/dbacademy/ops/labuser12696561_1763456832@vocareum_com\nDataset Location:  NestedNamespace (arxiv='/Volumes/dbacademy_arxiv/v01', dais='/Volumes/dbacademy_dais/v01', news='/Volumes/dbacademy_news/v01', docs='/Volumes/dbacademy_docs/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42022f04-4c07-4094-a378-f9f8bccd518e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo Overview\n",
    "\n",
    "As seen in the diagram below, in this demo we will focus on the inference section (highlighted in green). The main focus of the previous demos was  Step 1 - Data preparation and vector storage. Now, it is time to put all components together to create a RAG application. \n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/genai/genai-as-01-llm-rag-self-managed-flow-2.png\" width=\"100%\"> -->\n",
    "\n",
    "<!--  -->\n",
    "\n",
    "![genai-as-01-llm-rag-self-managed-flow-2](../Includes/images/genai-as-01-llm-rag-self-managed-flow-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45f1b50b-6c1c-4b51-86bf-7cf6cb62cab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup the RAG Components\n",
    "\n",
    "In this section, we will first define the components that we created before. Next, we will set up the retriever component for the application. Then, we will combine all the components together. In the final step, we will register the developed application as a model in the Model Registry with Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb4a019-bd42-4af1-91bb-c4390aaa09be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup the Retriever\n",
    "\n",
    "We will setup the Vector Search endpoint that we created in the previous demos as retriever. The retriever will return 2 relevant documents based on the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6f5ca2-5c52-43f5-a8c2-b99b6b857816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Vector Search endpoint name: vs_endpoint_1.\n"
     ]
    }
   ],
   "source": [
    "# components we created before\n",
    "# assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "\n",
    "vs_endpoint_name = vs_endpoint_prefix + str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")\n",
    "\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_self_managed_vs_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27068aa-b9b6-4912-b6f8-7404a7c2f9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763459662.658229    7034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763459662.664279    7034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1763459662.682814    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1763459662.682833    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1763459662.682836    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1763459662.682838    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-18 09:54:25,180] [WARNING] [real_accelerator.py:194:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n[2025-11-18 09:54:25,183] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\nRelevant documents: [Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.07842.pdf'}, page_content='Such data cannot be used in a su-\\npervised setting, but can provide rewards in the context of R einforcement Learning (RL) ( Sutton and Barto ,\\n2018).\\nRL has proven successful for learning complex behaviors thr ough feedback-based interaction with an en-\\nvironment, and it has been us for applications such as playin g games ( Mnih et al. ,2015;Silver et al. ,\\n2016;Vinyals et al. ,2019;Team et al. ,2021;Bakhtin et al. ,2022) or controlling robots ( Gu et al. ,2017;\\nKalashnikov et al. ,2018;Akkaya et al. ,2019;Lee et al. ,2020). When training a LM with RL, the LM can\\nbe considered an agent that learns a policy (i.e. a distribut ion over the model’s vocabulary from which the\\nnext token is sampled) in order to optimize some reward funct ion. Most of the existing work on RL and ALMs\\nhas focused on teaching LMs how to act rather than reason. The closest work on learning how to reason via\\nRL is STaR ( Zelikman et al. ,2022), a bootstrapping-based approach that is discussed in Sect ion4.1\\nRL is a natural framework for training LMs to act and use tools since many of these tools are non-\\ndiﬀerentiable (e.g. search engines, calculators or progra mming language interpreters). Additionally, many\\ntasks that beneﬁt from interacting with tools resemble sequ ential decision making problems (e.g., navigat-\\ning a web-browser to buy a speciﬁed product) and have a well-d eﬁned reward (e.g., 1 if the model buys\\nthe correct product and 0 otherwise).'), Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf'}, page_content='Deﬁnition 7. Inductive Learning , which is the most common setting in machine learning tasks, trains the\\nmodel on labeled data and then tests on samples that have never appeared in the training stage. For-\\nmally, given a training sample f(ci;Gci;yi)gNl\\ni=1,f(cj;Gcj)gNu\\nj=1, whereNlandNuare the numbers of\\nlabeled/unlabeled samples. Inductive learning learns a function find:G7!Y so thatfindis expected to\\nbe a good classiﬁer on the future graph data f(ck;Gck)g, beyondf(cj;Gcj)gNu\\nj=1.\\nDeﬁnition 8. Transductive Learning is different from inductive learning in that all samples are visi-\\nble during both the training and testing stages. Formally, given a training sample f(ci;Gci;yi)gNl\\ni=1,\\nf(cj;Gcj)gNu\\nj=1, transductive learning learns a function ftrans:Gl+u7!Yl+uso thatftransis expected to\\nbe a good classiﬁer on the unlabeled data f(cj;Gcj)gNu\\nj=1.\\nUnder the supervised setting (including semi-/self-supervised), the uniﬁed classiﬁer optimization meth-\\nods of inductive learning and transductive learning can be written as:\\nL=1\\nKKX\\ni=1L(f(\\x01)\\n\\x12(ci;Gci);yi); (19)\\nwhereLis the cross-entropy loss, cican be node, edge or subgraph of its associated graph Gci, andf(\\x01)\\n\\x12\\ndenotes inductive/transductive function with parameter \\x12.\\nCompared with using only one pretext task, some methods have designed some integration mechanisms\\nto incorporate the advantages of multiple pretext tasks into a uniﬁed framework.')]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-6f4c03cf4c834326f859143ef7ca0fe9\"",
      "text/plain": [
       "Trace(trace_id=tr-6f4c03cf4c834326f859143ef7ca0fe9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "def get_retriever(cache_dir=\"/tmp\"):\n",
    "\n",
    "    def retrieve(query, k: int=10):\n",
    "        if isinstance(query, dict):\n",
    "            query = next(iter(query.values()))\n",
    "\n",
    "        # get the vector search index\n",
    "        vsc = VectorSearchClient(disable_notice=True)\n",
    "        vs_index = vsc.get_index(endpoint_name=vs_endpoint_name, index_name=vs_index_fullname)\n",
    "        \n",
    "        # get the query vector\n",
    "        embeddings = DatabricksEmbeddings(endpoint=\"databricks-gte-large-en\")\n",
    "        query_vector = embeddings.embed_query(query)\n",
    "        \n",
    "        # get similar k documents\n",
    "        return query, vs_index.similarity_search(\n",
    "            query_vector=query_vector,\n",
    "            columns=[\"pdf_name\", \"content\"],\n",
    "            num_results=k)\n",
    "\n",
    "    def rerank(query, retrieved, cache_dir, k: int=2):\n",
    "        # format result to align with reranker lib format \n",
    "        passages = []\n",
    "        for doc in retrieved.get(\"result\", {}).get(\"data_array\", []):\n",
    "            new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "            passages.append(new_doc)       \n",
    "        # Load the flashrank ranker\n",
    "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=cache_dir)\n",
    "\n",
    "        # rerank the retrieved documents\n",
    "        rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "        results = ranker.rerank(rerankrequest)[:k]\n",
    "\n",
    "        # format the results of rerank to be ready for prompt\n",
    "        return [Document(page_content=r.get(\"text\"), metadata={\"source\": r.get(\"file\")}) for r in results]\n",
    "\n",
    "    # the retriever is a runnable sequence of retrieving and reranking.\n",
    "    return RunnableLambda(retrieve) | RunnableLambda(lambda x: rerank(x[0], x[1], cache_dir))\n",
    "\n",
    "# test our retriever\n",
    "question = {\"input\": \"What is Inductive Learning?\"}\n",
    "vectorstore = get_retriever(cache_dir = f\"{DA.paths.working_dir}/opt\")\n",
    "similar_documents = vectorstore.invoke(question)\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75963ea6-f8ba-4507-96d9-0a785e2c99f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup the Foundation Model\n",
    "\n",
    "Our chatbot will be using `llama3.3` foundation model to provide answer. \n",
    "\n",
    "While the model is available using the built-in [Foundation endpoint](/ml/endpoints), we can use Databricks Langchain Chat Model wrapper to easily build our chain.  \n",
    "\n",
    "Note: multiple type of endpoint or langchain models can be used.\n",
    "\n",
    "- Databricks Foundation models (what we'll use)\n",
    "- Your fined-tune model\n",
    "- An external model provider (such as Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484e7ff5-b929-4757-bb83-48f69deed9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://oregon.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chat model: content='Generative AI refers to a type of artificial intelligence that is capable of generating new, original content, such as images, videos, music, text, or even entire data sets. This is in contrast to traditional AI, which is typically used for classification, regression, or other predictive tasks.\\n\\nGenerative AI models use complex algorithms and statistical techniques to learn patterns and relationships in data, and then generate new data that is similar in style, structure, and content to the original data. These models can be trained on large datasets of images, text, or other types of data, and can learn to generate new examples that are often indistinguishable from real data.\\n\\nSome common applications of generative AI include:\\n\\n1. **Image and video generation**: Generative AI can be used to generate realistic images and videos, such as faces, landscapes, or objects.\\n2. **Text generation**: Generative AI can be used to generate text, such as articles, stories, or chatbot responses.\\n3. **Music generation**: Generative AI can be used to generate music, such as melodies, harmonies, or entire songs.\\n4. **Data augmentation**: Generative AI can be used to generate new data that can be used to augment existing datasets, which can help to improve the performance of machine learning models.\\n5. **Art and design**: Generative AI can be used to generate original art, such as paintings, sculptures, or architectural designs.\\n\\nSome popular types of generative AI models include:\\n\\n' additional_kwargs={} response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 300, 'total_tokens': 316}, 'prompt_tokens': 16, 'completion_tokens': 300, 'total_tokens': 316, 'model': 'meta-llama-3.3-70b-instruct-121024', 'model_name': 'meta-llama-3.3-70b-instruct-121024', 'finish_reason': 'length'} id='run--08b9bb12-35ff-4218-9f61-a6c39cd4ec79-0'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-b5f57d4f9f01491cae76478bd8f55988\"",
      "text/plain": [
       "Trace(trace_id=tr-b5f57d4f9f01491cae76478bd8f55988)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# test Databricks Foundation LLM model\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7beab663-638c-48f2-b9cb-a6f93ffdcb33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assembling the Complete RAG Solution\n",
    "\n",
    "Let's now merge the retriever and the model in a single Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/genai/genai-as-01-llm-rag-self-managed-model-2.png\" width=\"100%\" /> -->\n",
    "\n",
    "![genai-as-01-llm-rag-self-managed-model-2](../Includes/images/genai-as-01-llm-rag-self-managed-model-2.png)\n",
    "\n",
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c441e3-0b74-4360-a6b4-799485fad05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some important notes about the LangChain formatting:\n",
    "\n",
    "* Context documents retrieved from the vector store are added by separated newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c061cea0-aaf9-4e56-9fa3-03a6daa6a2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "TEMPLATE = \"\"\"You are an assistant for GENAI teaching class. You are answering questions related to Generative AI and how it impacts humans life. If the question is not related to one of these topics, kindly decline to answer. \n",
    "Use the following pieces of context to answer the question at the end:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(TEMPLATE)      \n",
    "\n",
    "# Helper functions\n",
    "def format_docs(docs):\n",
    "    # what the model sees in {context}\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "def unwrap(payload):\n",
    "    # return both answer and normalized context (dicts) like you wanted\n",
    "    docs = payload[\"docs\"]\n",
    "    return {\n",
    "        \"answer\": payload[\"answer\"],\n",
    "        \"context\": [{\"metadata\": getattr(d, \"metadata\", {}), \"page_content\": getattr(d, \"page_content\", \"\")}\n",
    "                    for d in docs],\n",
    "    }\n",
    "\n",
    "# ---- build the chain ----\n",
    "# Step 1: retrieve docs\n",
    "retrieve = RunnableParallel(input=RunnablePassthrough(), docs=get_retriever())\n",
    "\n",
    "# Step 2: pass formatted context + input to the model\n",
    "rag = retrieve | {\n",
    "    \"input\": itemgetter(\"input\"),\n",
    "    \"context\": RunnableLambda(lambda x: format_docs(x[\"docs\"]))\n",
    "} | prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Keep docs for postprocessing\n",
    "chain = retrieve | {\n",
    "    \"answer\": ({\"input\": itemgetter(\"input\"), \"context\": RunnableLambda(lambda x: format_docs(x[\"docs\"]))}\n",
    "               | prompt | chat_model | StrOutputParser()),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "} | RunnableLambda(unwrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b9c016-5c0e-4ab1-b6a2-5f27c6d9b6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading rank-T5-flan...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rrank-T5-flan.zip:   0%|          | 0.00/73.7M [00:00<?, ?iB/s]\rrank-T5-flan.zip:  21%|██        | 15.2M/73.7M [00:00<00:00, 159MiB/s]\rrank-T5-flan.zip:  55%|█████▌    | 40.6M/73.7M [00:00<00:00, 222MiB/s]\rrank-T5-flan.zip:  90%|█████████ | 66.5M/73.7M [00:00<00:00, 244MiB/s]\rrank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:00<00:00, 230MiB/s]\nINFO:httpx:HTTP Request: POST https://oregon.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economical implications of Generative AI, such as GPT-4, are likely to be pervasive. While these models have consistently improved in capabilities over time, their growing economic effect is expected to persist and increase even if we halt the development of new capabilities today. Studies suggest that the potential impact of Generative AI expands significantly when we take into account the development of complementary technologies. \n\nThe exposure to Generative AI varies by industry, with information processing industries exhibiting high exposure, while manufacturing, agriculture, and mining demonstrate lower exposure. The connection between productivity growth in the past decade and overall Generative AI exposure appears weak, suggesting a potential optimistic case that future productivity gains from Generative AI may not exacerbate possible cost disease effects.\n\nResearch indicates that the variance explained by previous technology exposure measurements ranges from 60 to 72%, leaving 28 to 40% of the variation in Generative AI exposure unaccounted for. This implies that there are still uncertainties and unexplored aspects of the economical implications of Generative AI.\n\nAdditionally, studies have explored the impact of Generative AI on income and wealth inequality, with some findings suggesting that automation's impact on these areas is uneven. Others have examined the effects of Generative AI on developer productivity, with evidence from GitHub Copilot suggesting a positive impact.\n\nOverall, the economical implications of Generative AI are complex, multifaceted, and require further research to fully understand their effects on various industries, productivity, and societal outcomes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-5785cb254c783ca0232c530879ee626f\"",
      "text/plain": [
       "Trace(trace_id=tr-5785cb254c783ca0232c530879ee626f)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = {\"input\": \"What are the generative AI's economical implications?\"}\n",
    "response = chain.invoke(question)\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f7ecbcf-7662-4ca2-811f-dcc74a69e83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save the Model to Model Registry in UC\n",
    "\n",
    "Now that our model is ready and evaluated, we can register it within our Unity Catalog schema. \n",
    "\n",
    "After registering the model, you can view the model and models in the **Catalog Explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab7e2a9-6d09-4bac-8a1d-7037191b0573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 View Logged Model at: https://dbc-5b9a7c32-5748.cloud.databricks.com/ml/experiments/3716130915455658/models/m-24e0f358b555469692d3c6b34e04df78?o=2367603887149683\n2025/11/18 09:55:37 INFO mlflow: Attempting to auto-detect Databricks resource dependencies for the current langchain model. Dependency auto-detection is best-effort and may not capture all dependencies of your langchain model, resulting in authorization errors when serving or querying your model. We recommend that you explicitly pass `resources` to mlflow.langchain.log_model() to ensure authorization to dependent resources succeeds when the model is deployed.\n2025/11/18 09:55:38 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-24e0f358b555469692d3c6b34e04df78\n2025/11/18 09:55:38 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://oregon.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nSuccessfully registered model 'dbacademy.labuser12696561_1763456832.rag_app_demo4'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb48c925bb6244cb915be23b99ca072c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '1' of model 'dbacademy.labuser12696561_1763456832.rag_app_demo4': https://dbc-5b9a7c32-5748.cloud.databricks.com/explore/data/models/dbacademy/labuser12696561_1763456832/rag_app_demo4/version/1?o=2367603887149683\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "# set model registry to UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_demo4\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"rag_app_demo4\") as run:\n",
    "    signature = infer_signature(question, response)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever, \n",
    "        name=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        signature=signature,\n",
    "        input_example=question\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a97548-e043-4b66-acf0-66009bf03ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Resources\n",
    "\n",
    "This is the final demo. You can delete all resources created in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419e4005-3a6a-430e-b612-39fa65687e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we illustrated the process of constructing a comprehensive RAG application utilizing a variety of Databricks products. Initially, we established the RAG components that were previously created in the earlier demos, namely the Vector Search endpoint and Vector Search index. Subsequently, we constructed the retriever component and set up the foundational model for use. Following this, we put together the entire RAG application and evaluated the performance of the pipeline using MLflow's LLM evaluation functions. As a final step, we registered the newly created RAG application as a model within the Model Registry with Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccbb3f6-d988-4e1c-b39c-dbc96cd432ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1 - Assembling and Registering a RAG Application",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}